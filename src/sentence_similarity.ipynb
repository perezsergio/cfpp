{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 18:36:21.388459: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-14 18:36:21.391139: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-14 18:36:21.422121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-14 18:36:22.164693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3242412713635586\n",
      "0.664081211527903\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "s1 = nlp(\"I don't know whether I want you \")\n",
    "s2 = nlp(\"This is not the weather. I warned you.\")\n",
    "s3 = nlp(\"Today is an awful rainy day and I told you so.\")\n",
    "\n",
    "# remove stop words\n",
    "s1 = nlp(\" \".join([str(t) for t in s1 if not t.is_stop]))\n",
    "s2 = nlp(\" \".join([str(t) for t in s2 if not t.is_stop]))\n",
    "s3 = nlp(\" \".join([str(t) for t in s3 if not t.is_stop]))\n",
    "\n",
    "print(s1.similarity(s2))\n",
    "print(s2.similarity(s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1356283932820694\n",
      "0.6167017143503872\n"
     ]
    }
   ],
   "source": [
    "import spacy_universal_sentence_encoder\n",
    "\n",
    "nlp = spacy_universal_sentence_encoder.load_model(\"en_use_lg\")\n",
    "\n",
    "s1 = nlp(\"I don't know whether I want you \")\n",
    "s2 = nlp(\"The weather has been terrible today. I warned you.\")\n",
    "s3 = nlp(\"Today is an awful rainy day and I told you so.\")\n",
    "\n",
    "print(s1.similarity(s2))\n",
    "print(s2.similarity(s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_universal_sentence_encoder\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def read_txt_files(directory_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read all txt files from target directory,\n",
    "    concat the text from all of them, return the full text.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    file_paths = glob(directory_path + \"/*.txt\")\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            all_text += file.read() + \"\\n\\n\"\n",
    "    return all_text\n",
    "\n",
    "\n",
    "# Define path of the directory that contains the temporary tmp\n",
    "txt_files_dir = \"../data/txt_documents\"\n",
    "text = read_txt_files(txt_files_dir)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "nlp = spacy_universal_sentence_encoder.load_model(\"en_use_lg\")\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "search_string = nlp(\"prerequisites for a new developer\")\n",
    "\n",
    "top_5 = {0: \"\"}\n",
    "max_val = 0\n",
    "for sentence in sentences:\n",
    "    similarity = nlp(sentence).similarity(search_string)\n",
    "    max_val = max(similarity, max_val)\n",
    "    if similarity < min(top_5.keys()):\n",
    "        continue\n",
    "    top_5[similarity] = sentence\n",
    "    if len(top_5.keys()) >= 6:\n",
    "        del top_5[min(top_5.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22785773603262818 A serious problem with previous simulation codes was\n",
      "the difficulty of adding new or variant physics models; development was difficult due to the increased size, complexity\n",
      "and interdependency of the procedure-based code.\n",
      "0.18352366592511993 Status of this Document\n",
      "An introduction to the Geant4 Toolkit.\n",
      "0.20855739322863934 New requirements, such as requests for new functionality, are presented to and decided by the Steering Board (SB).\n",
      "0.18153792884253764 A firm knowledge of C++ is required to implement\n",
      "code in user action classes to specify, at a minimum, the detector description, the relevant particles and physics\n",
      "processes, and the initial event kinematics.\n",
      "0.2597938138943387 This requires the development of new classes overloading standard Geant4 functionality and\n",
      "hence a solid understanding of object-oriented Programming.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A firm knowledge of C++ is required to implement\\ncode in user action classes to specify, at a minimum, the detector description, the relevant particles and physics\\nprocesses, and the initial event kinematics.',\n",
       " 'Status of this Document\\nAn introduction to the Geant4 Toolkit.',\n",
       " 'New requirements, such as requests for new functionality, are presented to and decided by the Steering Board (SB).',\n",
       " 'A serious problem with previous simulation codes was\\nthe difficulty of adding new or variant physics models; development was difficult due to the increased size, complexity\\nand interdependency of the procedure-based code.',\n",
       " 'This requires the development of new classes overloading standard Geant4 functionality and\\nhence a solid understanding of object-oriented Programming.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy.lang\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "def get_top5(search_string: str, sentences: list[str], nlp: English):\n",
    "    # top 5 sentences with higher similarity will be stored in a dictionary with the format\n",
    "    # {similarity: sentence}\n",
    "    top_5 = {0: \"\"}\n",
    "    for sentence in sentences:\n",
    "        # Compute similarity using nlp\n",
    "        similarity = nlp(sentence).similarity(nlp(search_string))\n",
    "        # If similarity is not big enough to enter in the top 5, continue to next sentence\n",
    "        if similarity < min(top_5.keys()):\n",
    "            continue\n",
    "        # Else, add current similarity, sentence to the top_5 dict\n",
    "        top_5[similarity] = sentence\n",
    "        # If the top_5 dict has more than 5 elements, delete the one with the smallest similarity\n",
    "        if len(top_5.keys()) > 5:\n",
    "            del top_5[min(top_5.keys())]\n",
    "    for k, v in top_5.items():\n",
    "        print(k, v)\n",
    "    # Return array with top 5 sentences, ordered from largest similarity\n",
    "    return [top_5[sim] for sim in sorted(top_5.keys())]\n",
    "\n",
    "\n",
    "search_string = \"prerequisites for a new developer\"\n",
    "get_top5(search_string, sentences, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy.lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_txt_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define path of the directory that contains the temporary tmp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m txt_files_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/txt_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mread_txt_files\u001b[49m(txt_files_dir)\n\u001b[1;32m      9\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[1;32m     12\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_txt_files' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy_universal_sentence_encoder\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# Define path of the directory that contains the temporary tmp\n",
    "txt_files_dir = \"../data/txt_documents\"\n",
    "text = read_txt_files(txt_files_dir)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "search_string = nlp(\"prerequisites for a new developer\")\n",
    "\n",
    "\n",
    "def get_top5(query_string: str, text: str):\n",
    "    top_5 = {0: \"\"}\n",
    "    max_val = 0\n",
    "    for sentence in sentences:\n",
    "        similarity = nlp(sentence).similarity(search_string)\n",
    "        max_val = max(similarity, max_val)\n",
    "        if similarity < min(top_5.keys()):\n",
    "            continue\n",
    "        top_5[similarity] = sentence\n",
    "        if len(top_5.keys()) >= 6:\n",
    "            del top_5[min(top_5.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtop_5\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key, val)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_5' is not defined"
     ]
    }
   ],
   "source": [
    "for key, val in top_5.items():\n",
    "    print(key, val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
